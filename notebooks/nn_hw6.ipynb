{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "nn_hw6.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4zxzAtJcuNi"
      },
      "source": [
        "# Домашнее задание 5\n",
        "\n",
        "Ссылка на семинар: https://colab.research.google.com/drive/1_G8kdmhtokmAUeMsSrl1IKqIs3_FEbeK?usp=sharing\n",
        "\n",
        "Не забудьте поставить видеокарту в качестве аппаратного ускорителя :)\n",
        "\n",
        "### Сохранение и загрузка\n",
        "\n",
        "[3 балла] \n",
        "\n",
        "Сделайте сохранение моделей после обучения (в формате .pth). В случае маленьких сетей на маленьких данных в этом нет необходимости, но в других случаях можно сохранять версию модели (checkpoint) каждые несколько (к примеру 5) эпох. Сделайте и протестируйте загрузку моделей для задачи классификации рукописных цифр.\n",
        "\n",
        "### Сверточная сеть на CIFAR-10\n",
        "\n",
        "[5 баллов]\n",
        "\n",
        "Решите задачу классификации на другом датасете - CIFAR-10 https://en.wikipedia.org/wiki/CIFAR-10\n",
        "В примерах к Pytorch есть полное решение этой задачи, но попробуйте придумать его самостоятельно. Этот датасет отличается от mnist тем, что изображения в нём имеют размер не 28x28, а 32x32x3, то есть они трёхканальные, \"цветные\". Для того, чтобы сеть заработала и начала хотя бы как-то учиться, нужно сделать так, чтобы размеры (shapes) тензоров на выходе предыдущего слоя совпадали с размером тензоров на входе следующего слоя.\n",
        "\n",
        "### Классификация на своих картинках\n",
        "\n",
        "[6 баллов]\n",
        "\n",
        "Запустите распознавание цифр на самостоятельно записанных данных. Можно использовать фотографию листочка, можно просто нарисовать их в пейнте. Чтобы все заработало, нужнро во-первых помнить, что сеть, созданная для работы с mnist, принимает на вход одноканальные картинки 28x28, а во-вторых учесть, что границы цифр в обучающих данных не резкие, в процессе их подготовки был применён антиалеасинг.\n",
        "\n",
        "### Загадка\n",
        "\n",
        "Доп.задача 1 [5 баллов]\n",
        "\n",
        "Найдите, почему при обучении свёрточной сети (Conv_net в семинаре) test loss всё время меньше train loss-а. Напишите, почему так получается, исправьте ошибку, если она есть. Обратите внимание, что при обучении Simple_net такого не наблюдается."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGOX-9fjcuNs"
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6hdKBtIm0Lh",
        "outputId": "ab498926-df78-4434-9865-4b39feabad3b"
      },
      "source": [
        "batch_size = 100\n",
        "no_cuda         = False\n",
        "use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT2aC3AVXMS_",
        "outputId": "6c66d294-73e1-4ebf-ae12-12a4a86d076e"
      },
      "source": [
        "for i, (data,target) in enumerate(train_loader):\n",
        "  print(data.shape)\n",
        "  print(type(target))\n",
        "  break"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 3, 32, 32])\n",
            "<class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0wBLq0YrDVJ"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, hidden, out_sz):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(28**2, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, hidden)\n",
        "        self.fc3 = nn.Linear(hidden, out_sz)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.leaky_relu (x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = F.leaky_relu (x)\n",
        "        \n",
        "        x = self.fc3(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "\n",
        "        return output\n",
        "\n",
        "class Conv_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv_net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, 1,padding=1)\n",
        "        self.act1=nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1,padding=1)\n",
        "        self.max_pool2=nn.MaxPool2d(2)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        #Flatten in forward function\n",
        "        self.fc1 = nn.Linear(16384, 128)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.act3 = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #torch.Size([64, 3, 32, 32])\n",
        "        x = self.conv1(x)\n",
        "        #torch.Size([64, 32, 32, 32])\n",
        "        x = self.act1(x)\n",
        "        x = self.conv2(x)\n",
        "        #torch.Size([64,64,32,32])\n",
        "        x = self.max_pool2(x)\n",
        "        #torch.Size([64, 64, 16, 16])\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        #torch.Size([64, 16384])\n",
        "        x = self.fc1(x)\n",
        "        #torch.Size([64, 128])\n",
        "        x = self.act2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        #torch.Size([64, 10])\n",
        "        #x = self.act3(x)\n",
        "        #torch.Size([64, 10])\n",
        "        return x"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuPHZ5IjPQb_"
      },
      "source": [
        "def train(network, train_dataset, epochs, criterion, optimizer):\n",
        "    loss_epochs = []\n",
        "    for idx in range(epochs):\n",
        "        loss_samples = []\n",
        "        for data,target in train_dataset:\n",
        "            data=data.to(device)\n",
        "            target=target.to(device)\n",
        "            optimizer.zero_grad()   # zero the gradient buffers\n",
        "            output = network(data)\n",
        "            loss = criterion(output, target.to(device))\n",
        "            loss.backward()\n",
        "            loss_samples.append(loss.data.cpu().numpy())\n",
        "            optimizer.step()    # Does the update\n",
        "\n",
        "        loss_samples_mean = float(sum(loss_samples)) / len (loss_samples)\n",
        "        print(f\"Epoch {idx: >8} Loss: {loss_samples_mean}\")\n",
        "        loss_epochs.append(loss_samples_mean)\n",
        "\n",
        "    plt.plot(loss_epochs)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.show() "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBy-nTGVRjdF"
      },
      "source": [
        "def test(network, test_dataset, dividing_criterion):\n",
        "    domik_x = []\n",
        "    domik_y = []\n",
        "    nedomik_x = []\n",
        "    nedomik_y = []\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sample in test_dataset:\n",
        "            inp, target = sample\n",
        "            output = network(inp)\n",
        "            total += 1\n",
        "            correct += (dividing_criterion(output) == dividing_criterion(target))\n",
        "\n",
        "            # dividing dataset for plotting\n",
        "            if dividing_criterion(output):\n",
        "                \n",
        "                domik_x.append(inp[0])\n",
        "                domik_y.append(inp[1])\n",
        "            else: \n",
        "                nedomik_x.append(inp[0])\n",
        "                nedomik_y.append(inp[1])\n",
        "    \n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(121)\n",
        "    test_dataset.show()\n",
        "    plt.subplot(122)\n",
        "    plt.plot(domik_x, domik_y, '.', nedomik_x, nedomik_y, '.')\n",
        "\n",
        "    print(f'Accuracy of the network on the {total} test samples: {100 * correct / total}')"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a6h-ghjRu1c",
        "outputId": "08c3d48b-84dc-45f6-ea05-3cfa256bd050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "net = Conv_net().to(device)\n",
        "EPOCHS_TO_TRAIN = 50\n",
        "train(network=net, \n",
        "      train_dataset=train_loader, \n",
        "      epochs=EPOCHS_TO_TRAIN, \n",
        "      criterion=nn.CrossEntropyLoss(), \n",
        "      optimizer=optim.Adam(net.parameters(), lr=0.001))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch        0 Loss: 1.508730169057846\n",
            "Epoch        1 Loss: 1.1753930938243866\n",
            "Epoch        2 Loss: 1.0483304728269578\n",
            "Epoch        3 Loss: 0.9516425919532776\n",
            "Epoch        4 Loss: 0.8875651352405548\n",
            "Epoch        5 Loss: 0.8197824597358704\n",
            "Epoch        6 Loss: 0.7709039905667305\n",
            "Epoch        7 Loss: 0.7307294963002204\n",
            "Epoch        8 Loss: 0.68124217659235\n",
            "Epoch        9 Loss: 0.6298384914398193\n",
            "Epoch       10 Loss: 0.5940608960986138\n",
            "Epoch       11 Loss: 0.5691893176436424\n",
            "Epoch       12 Loss: 0.5382691284418106\n",
            "Epoch       13 Loss: 0.5145950546264648\n",
            "Epoch       14 Loss: 0.4839957937002182\n",
            "Epoch       15 Loss: 0.4678484289050102\n",
            "Epoch       16 Loss: 0.45377194428443907\n",
            "Epoch       17 Loss: 0.43790816980600356\n",
            "Epoch       18 Loss: 0.4279299075603485\n",
            "Epoch       19 Loss: 0.40227016305923463\n",
            "Epoch       20 Loss: 0.3993542747497559\n",
            "Epoch       21 Loss: 0.38513504034280777\n",
            "Epoch       22 Loss: 0.3761931505203247\n",
            "Epoch       23 Loss: 0.3664820244908333\n",
            "Epoch       24 Loss: 0.3627102637887001\n",
            "Epoch       25 Loss: 0.3505972473621368\n",
            "Epoch       26 Loss: 0.3483511188030243\n",
            "Epoch       27 Loss: 0.3299803180247545\n",
            "Epoch       28 Loss: 0.328520055770874\n",
            "Epoch       29 Loss: 0.32590630346536636\n",
            "Epoch       30 Loss: 0.31752502965927126\n",
            "Epoch       31 Loss: 0.3124673352390528\n",
            "Epoch       32 Loss: 0.3020013763606548\n",
            "Epoch       33 Loss: 0.3053562322854996\n",
            "Epoch       34 Loss: 0.3065481368452311\n",
            "Epoch       35 Loss: 0.28827940912544725\n",
            "Epoch       36 Loss: 0.2923573147803545\n",
            "Epoch       37 Loss: 0.2706520167812705\n",
            "Epoch       38 Loss: 0.28174734506011007\n",
            "Epoch       39 Loss: 0.28012905295193197\n",
            "Epoch       40 Loss: 0.2781889997422695\n",
            "Epoch       41 Loss: 0.27826546688377857\n",
            "Epoch       42 Loss: 0.27347526079416273\n",
            "Epoch       43 Loss: 0.270101768925786\n",
            "Epoch       44 Loss: 0.27012894731760023\n",
            "Epoch       45 Loss: 0.26047371643781664\n",
            "Epoch       46 Loss: 0.25719777746498584\n",
            "Epoch       47 Loss: 0.2580315166860819\n",
            "Epoch       48 Loss: 0.25080055175721644\n",
            "Epoch       49 Loss: 0.25461434035003183\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OPQfkYcOEf3"
      },
      "source": [
        "#**train** and **test** functions that I don't understand"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyER4Pl7bPjI"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval, loss_archive):\n",
        "    train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = nn.CrossEntropyLoss()\n",
        "        loss= loss(output,target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item ()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "    train_loss = train_loss / len(train_loader)\n",
        "    loss_archive.append (train_loss)\n",
        "\n",
        "def test(model, device, test_loader, loss_archive):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target).item()  # sum up batch loss #\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss = test_loss / len(test_loader)\n",
        "    loss_archive.append (test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xZC5-kLuboDs",
        "outputId": "59f94095-0d3a-4923-adf7-29fb37041f26"
      },
      "source": [
        "epochs          = 100\n",
        "lr              = 0.1\n",
        "gamma           = 0.7\n",
        "seed            = 1\n",
        "log_interval    = 100\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "#model = Simple_net(100, 10).to(device)\n",
        "model = Conv_net().to(device)\n",
        "\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
        "\n",
        "train_loss = []\n",
        "test_loss  = []\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch, log_interval, train_loss)\n",
        "    test(model, device, test_loader, test_loss)\n",
        "    scheduler.step()\n",
        "\n",
        "    #plt.figure (figsize=(24, 15))\n",
        "    \n",
        "    #plt.plot (train_loss, 'r')\n",
        "    #plt.plot (test_loss, 'g')\n",
        "    #plt.ylim(ymin=0)\n",
        "\n",
        "    #plt.show ()\n",
        "\n",
        "    #plt.pause (0.01)\n",
        "    #clear_output(wait = True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.303771\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.224361\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.215011\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 2.239800\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 2.178665\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.136607\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 2.174968\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 2.071078\n",
            "\n",
            "Test set: Average loss: -0.3557, Accuracy: 3793/10000 (38%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.065658\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 2.148030\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 2.115513\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 2.056143\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 2.098311\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 2.050223\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 2.103253\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 2.002676\n",
            "\n",
            "Test set: Average loss: -0.3993, Accuracy: 4167/10000 (42%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 2.026276\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 2.058887\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 2.024312\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 2.066121\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 2.097334\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 2.096445\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 2.021570\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 2.065577\n",
            "\n",
            "Test set: Average loss: -0.4200, Accuracy: 4423/10000 (44%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 2.008390\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 2.065969\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 2.013404\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 1.987042\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 2.017184\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 2.051817\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 2.054211\n",
            "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 1.993464\n",
            "\n",
            "Test set: Average loss: -0.4376, Accuracy: 4583/10000 (46%)\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 2.089526\n",
            "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 2.083385\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 2.044310\n",
            "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 2.042893\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.975021\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.975604\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 2.018166\n",
            "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 2.044245\n",
            "\n",
            "Test set: Average loss: -0.4434, Accuracy: 4698/10000 (47%)\n",
            "\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.939990\n",
            "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 2.042701\n",
            "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 2.105882\n",
            "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 1.946519\n",
            "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 1.993906\n",
            "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 1.927280\n",
            "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 2.103890\n",
            "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 2.034536\n",
            "\n",
            "Test set: Average loss: -0.4510, Accuracy: 4704/10000 (47%)\n",
            "\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: 2.001529\n",
            "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 1.900334\n",
            "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 2.030487\n",
            "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 2.056235\n",
            "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 1.973431\n",
            "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 2.028649\n",
            "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 2.005822\n",
            "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 1.981013\n",
            "\n",
            "Test set: Average loss: -0.4570, Accuracy: 4798/10000 (48%)\n",
            "\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.983298\n",
            "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 1.996778\n",
            "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 1.974587\n",
            "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 2.045103\n",
            "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 2.047327\n",
            "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 1.943605\n",
            "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 1.944766\n",
            "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 1.994001\n",
            "\n",
            "Test set: Average loss: -0.4590, Accuracy: 4842/10000 (48%)\n",
            "\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.984630\n",
            "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 2.087642\n",
            "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 1.930931\n",
            "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 2.074262\n",
            "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 1.986460\n",
            "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 1.977780\n",
            "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 1.956100\n",
            "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 1.992678\n",
            "\n",
            "Test set: Average loss: -0.4633, Accuracy: 4880/10000 (49%)\n",
            "\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: 2.009313\n",
            "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 2.006705\n",
            "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 1.920423\n",
            "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 2.053786\n",
            "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 2.021861\n",
            "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.932127\n",
            "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 2.059290\n",
            "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 1.967625\n",
            "\n",
            "Test set: Average loss: -0.4630, Accuracy: 4877/10000 (49%)\n",
            "\n",
            "Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.933671\n",
            "Train Epoch: 11 [6400/50000 (13%)]\tLoss: 2.046452\n",
            "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 1.995490\n",
            "Train Epoch: 11 [19200/50000 (38%)]\tLoss: 1.899367\n",
            "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 2.038784\n",
            "Train Epoch: 11 [32000/50000 (64%)]\tLoss: 2.015519\n",
            "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 1.917163\n",
            "Train Epoch: 11 [44800/50000 (90%)]\tLoss: 1.946172\n",
            "\n",
            "Test set: Average loss: -0.4640, Accuracy: 4880/10000 (49%)\n",
            "\n",
            "Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.955923\n",
            "Train Epoch: 12 [6400/50000 (13%)]\tLoss: 2.001308\n",
            "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 2.008734\n",
            "Train Epoch: 12 [19200/50000 (38%)]\tLoss: 2.004831\n",
            "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 2.033047\n",
            "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 1.998551\n",
            "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 2.026691\n",
            "Train Epoch: 12 [44800/50000 (90%)]\tLoss: 1.957014\n",
            "\n",
            "Test set: Average loss: -0.4658, Accuracy: 4885/10000 (49%)\n",
            "\n",
            "Train Epoch: 13 [0/50000 (0%)]\tLoss: 1.953756\n",
            "Train Epoch: 13 [6400/50000 (13%)]\tLoss: 2.001265\n",
            "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 1.978089\n",
            "Train Epoch: 13 [19200/50000 (38%)]\tLoss: 2.055222\n",
            "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 1.954560\n",
            "Train Epoch: 13 [32000/50000 (64%)]\tLoss: 1.940785\n",
            "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 2.070203\n",
            "Train Epoch: 13 [44800/50000 (90%)]\tLoss: 1.976657\n",
            "\n",
            "Test set: Average loss: -0.4660, Accuracy: 4890/10000 (49%)\n",
            "\n",
            "Train Epoch: 14 [0/50000 (0%)]\tLoss: 2.031463\n",
            "Train Epoch: 14 [6400/50000 (13%)]\tLoss: 1.948107\n",
            "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 1.920961\n",
            "Train Epoch: 14 [19200/50000 (38%)]\tLoss: 2.038583\n",
            "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 2.023130\n",
            "Train Epoch: 14 [32000/50000 (64%)]\tLoss: 2.059326\n",
            "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 2.044880\n",
            "Train Epoch: 14 [44800/50000 (90%)]\tLoss: 1.970718\n",
            "\n",
            "Test set: Average loss: -0.4665, Accuracy: 4884/10000 (49%)\n",
            "\n",
            "Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.927815\n",
            "Train Epoch: 15 [6400/50000 (13%)]\tLoss: 1.966134\n",
            "Train Epoch: 15 [12800/50000 (26%)]\tLoss: 1.954853\n",
            "Train Epoch: 15 [19200/50000 (38%)]\tLoss: 2.060977\n",
            "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 1.942750\n",
            "Train Epoch: 15 [32000/50000 (64%)]\tLoss: 2.014014\n",
            "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 2.013918\n",
            "Train Epoch: 15 [44800/50000 (90%)]\tLoss: 2.066404\n",
            "\n",
            "Test set: Average loss: -0.4674, Accuracy: 4889/10000 (49%)\n",
            "\n",
            "Train Epoch: 16 [0/50000 (0%)]\tLoss: 1.939581\n",
            "Train Epoch: 16 [6400/50000 (13%)]\tLoss: 2.017445\n",
            "Train Epoch: 16 [12800/50000 (26%)]\tLoss: 2.070199\n",
            "Train Epoch: 16 [19200/50000 (38%)]\tLoss: 2.032078\n",
            "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 1.982627\n",
            "Train Epoch: 16 [32000/50000 (64%)]\tLoss: 2.034709\n",
            "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 1.947017\n",
            "Train Epoch: 16 [44800/50000 (90%)]\tLoss: 1.907122\n",
            "\n",
            "Test set: Average loss: -0.4678, Accuracy: 4901/10000 (49%)\n",
            "\n",
            "Train Epoch: 17 [0/50000 (0%)]\tLoss: 1.916607\n",
            "Train Epoch: 17 [6400/50000 (13%)]\tLoss: 1.952373\n",
            "Train Epoch: 17 [12800/50000 (26%)]\tLoss: 1.948053\n",
            "Train Epoch: 17 [19200/50000 (38%)]\tLoss: 1.978066\n",
            "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 2.006535\n",
            "Train Epoch: 17 [32000/50000 (64%)]\tLoss: 1.963951\n",
            "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 2.008436\n",
            "Train Epoch: 17 [44800/50000 (90%)]\tLoss: 2.026420\n",
            "\n",
            "Test set: Average loss: -0.4667, Accuracy: 4902/10000 (49%)\n",
            "\n",
            "Train Epoch: 18 [0/50000 (0%)]\tLoss: 1.931980\n",
            "Train Epoch: 18 [6400/50000 (13%)]\tLoss: 2.005858\n",
            "Train Epoch: 18 [12800/50000 (26%)]\tLoss: 1.994366\n",
            "Train Epoch: 18 [19200/50000 (38%)]\tLoss: 1.890263\n",
            "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 1.952128\n",
            "Train Epoch: 18 [32000/50000 (64%)]\tLoss: 2.021477\n",
            "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 2.029723\n",
            "Train Epoch: 18 [44800/50000 (90%)]\tLoss: 1.987559\n",
            "\n",
            "Test set: Average loss: -0.4676, Accuracy: 4900/10000 (49%)\n",
            "\n",
            "Train Epoch: 19 [0/50000 (0%)]\tLoss: 1.940629\n",
            "Train Epoch: 19 [6400/50000 (13%)]\tLoss: 1.883151\n",
            "Train Epoch: 19 [12800/50000 (26%)]\tLoss: 1.971412\n",
            "Train Epoch: 19 [19200/50000 (38%)]\tLoss: 1.999780\n",
            "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 1.986479\n",
            "Train Epoch: 19 [32000/50000 (64%)]\tLoss: 2.007865\n",
            "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 1.832844\n",
            "Train Epoch: 19 [44800/50000 (90%)]\tLoss: 1.958171\n",
            "\n",
            "Test set: Average loss: -0.4672, Accuracy: 4905/10000 (49%)\n",
            "\n",
            "Train Epoch: 20 [0/50000 (0%)]\tLoss: 2.069633\n",
            "Train Epoch: 20 [6400/50000 (13%)]\tLoss: 1.960119\n",
            "Train Epoch: 20 [12800/50000 (26%)]\tLoss: 1.937717\n",
            "Train Epoch: 20 [19200/50000 (38%)]\tLoss: 2.003216\n",
            "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 2.038084\n",
            "Train Epoch: 20 [32000/50000 (64%)]\tLoss: 1.965185\n",
            "Train Epoch: 20 [38400/50000 (77%)]\tLoss: 2.010419\n",
            "Train Epoch: 20 [44800/50000 (90%)]\tLoss: 2.071712\n",
            "\n",
            "Test set: Average loss: -0.4669, Accuracy: 4902/10000 (49%)\n",
            "\n",
            "Train Epoch: 21 [0/50000 (0%)]\tLoss: 1.984645\n",
            "Train Epoch: 21 [6400/50000 (13%)]\tLoss: 1.927436\n",
            "Train Epoch: 21 [12800/50000 (26%)]\tLoss: 2.079266\n",
            "Train Epoch: 21 [19200/50000 (38%)]\tLoss: 2.006769\n",
            "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 1.915898\n",
            "Train Epoch: 21 [32000/50000 (64%)]\tLoss: 1.999425\n",
            "Train Epoch: 21 [38400/50000 (77%)]\tLoss: 1.933466\n",
            "Train Epoch: 21 [44800/50000 (90%)]\tLoss: 1.887934\n",
            "\n",
            "Test set: Average loss: -0.4670, Accuracy: 4903/10000 (49%)\n",
            "\n",
            "Train Epoch: 22 [0/50000 (0%)]\tLoss: 1.982264\n",
            "Train Epoch: 22 [6400/50000 (13%)]\tLoss: 2.030988\n",
            "Train Epoch: 22 [12800/50000 (26%)]\tLoss: 1.958022\n",
            "Train Epoch: 22 [19200/50000 (38%)]\tLoss: 2.008007\n",
            "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 2.013291\n",
            "Train Epoch: 22 [32000/50000 (64%)]\tLoss: 1.958099\n",
            "Train Epoch: 22 [38400/50000 (77%)]\tLoss: 1.908715\n",
            "Train Epoch: 22 [44800/50000 (90%)]\tLoss: 1.901386\n",
            "\n",
            "Test set: Average loss: -0.4668, Accuracy: 4896/10000 (49%)\n",
            "\n",
            "Train Epoch: 23 [0/50000 (0%)]\tLoss: 1.977253\n",
            "Train Epoch: 23 [6400/50000 (13%)]\tLoss: 1.964521\n",
            "Train Epoch: 23 [12800/50000 (26%)]\tLoss: 2.033845\n",
            "Train Epoch: 23 [19200/50000 (38%)]\tLoss: 1.967092\n",
            "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 1.912316\n",
            "Train Epoch: 23 [32000/50000 (64%)]\tLoss: 1.977866\n",
            "Train Epoch: 23 [38400/50000 (77%)]\tLoss: 1.983415\n",
            "Train Epoch: 23 [44800/50000 (90%)]\tLoss: 2.033294\n",
            "\n",
            "Test set: Average loss: -0.4661, Accuracy: 4900/10000 (49%)\n",
            "\n",
            "Train Epoch: 24 [0/50000 (0%)]\tLoss: 2.055905\n",
            "Train Epoch: 24 [6400/50000 (13%)]\tLoss: 1.991990\n",
            "Train Epoch: 24 [12800/50000 (26%)]\tLoss: 2.095861\n",
            "Train Epoch: 24 [19200/50000 (38%)]\tLoss: 2.006001\n",
            "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 2.067419\n",
            "Train Epoch: 24 [32000/50000 (64%)]\tLoss: 1.993508\n",
            "Train Epoch: 24 [38400/50000 (77%)]\tLoss: 2.016257\n",
            "Train Epoch: 24 [44800/50000 (90%)]\tLoss: 2.044605\n",
            "\n",
            "Test set: Average loss: -0.4668, Accuracy: 4897/10000 (49%)\n",
            "\n",
            "Train Epoch: 25 [0/50000 (0%)]\tLoss: 2.026177\n",
            "Train Epoch: 25 [6400/50000 (13%)]\tLoss: 1.883917\n",
            "Train Epoch: 25 [12800/50000 (26%)]\tLoss: 2.002142\n",
            "Train Epoch: 25 [19200/50000 (38%)]\tLoss: 1.941107\n",
            "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 2.010824\n",
            "Train Epoch: 25 [32000/50000 (64%)]\tLoss: 2.005925\n",
            "Train Epoch: 25 [38400/50000 (77%)]\tLoss: 2.087552\n",
            "Train Epoch: 25 [44800/50000 (90%)]\tLoss: 1.938264\n",
            "\n",
            "Test set: Average loss: -0.4676, Accuracy: 4898/10000 (49%)\n",
            "\n",
            "Train Epoch: 26 [0/50000 (0%)]\tLoss: 1.912122\n",
            "Train Epoch: 26 [6400/50000 (13%)]\tLoss: 1.869903\n",
            "Train Epoch: 26 [12800/50000 (26%)]\tLoss: 1.936649\n",
            "Train Epoch: 26 [19200/50000 (38%)]\tLoss: 1.955683\n",
            "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 1.927842\n",
            "Train Epoch: 26 [32000/50000 (64%)]\tLoss: 2.081847\n",
            "Train Epoch: 26 [38400/50000 (77%)]\tLoss: 2.028807\n",
            "Train Epoch: 26 [44800/50000 (90%)]\tLoss: 2.045277\n",
            "\n",
            "Test set: Average loss: -0.4674, Accuracy: 4897/10000 (49%)\n",
            "\n",
            "Train Epoch: 27 [0/50000 (0%)]\tLoss: 2.036222\n",
            "Train Epoch: 27 [6400/50000 (13%)]\tLoss: 2.009576\n",
            "Train Epoch: 27 [12800/50000 (26%)]\tLoss: 1.953564\n",
            "Train Epoch: 27 [19200/50000 (38%)]\tLoss: 1.925902\n",
            "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 1.962740\n",
            "Train Epoch: 27 [32000/50000 (64%)]\tLoss: 1.967197\n",
            "Train Epoch: 27 [38400/50000 (77%)]\tLoss: 2.049507\n",
            "Train Epoch: 27 [44800/50000 (90%)]\tLoss: 2.006492\n",
            "\n",
            "Test set: Average loss: -0.4675, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 28 [0/50000 (0%)]\tLoss: 2.058388\n",
            "Train Epoch: 28 [6400/50000 (13%)]\tLoss: 2.036507\n",
            "Train Epoch: 28 [12800/50000 (26%)]\tLoss: 2.059263\n",
            "Train Epoch: 28 [19200/50000 (38%)]\tLoss: 2.013986\n",
            "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 1.948740\n",
            "Train Epoch: 28 [32000/50000 (64%)]\tLoss: 2.012632\n",
            "Train Epoch: 28 [38400/50000 (77%)]\tLoss: 2.037774\n",
            "Train Epoch: 28 [44800/50000 (90%)]\tLoss: 2.045118\n",
            "\n",
            "Test set: Average loss: -0.4672, Accuracy: 4897/10000 (49%)\n",
            "\n",
            "Train Epoch: 29 [0/50000 (0%)]\tLoss: 2.037105\n",
            "Train Epoch: 29 [6400/50000 (13%)]\tLoss: 2.012779\n",
            "Train Epoch: 29 [12800/50000 (26%)]\tLoss: 2.025331\n",
            "Train Epoch: 29 [19200/50000 (38%)]\tLoss: 1.924713\n",
            "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 2.003032\n",
            "Train Epoch: 29 [32000/50000 (64%)]\tLoss: 1.982239\n",
            "Train Epoch: 29 [38400/50000 (77%)]\tLoss: 2.000895\n",
            "Train Epoch: 29 [44800/50000 (90%)]\tLoss: 1.880185\n",
            "\n",
            "Test set: Average loss: -0.4670, Accuracy: 4898/10000 (49%)\n",
            "\n",
            "Train Epoch: 30 [0/50000 (0%)]\tLoss: 2.067383\n",
            "Train Epoch: 30 [6400/50000 (13%)]\tLoss: 2.021041\n",
            "Train Epoch: 30 [12800/50000 (26%)]\tLoss: 1.925025\n",
            "Train Epoch: 30 [19200/50000 (38%)]\tLoss: 1.900144\n",
            "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 1.953236\n",
            "Train Epoch: 30 [32000/50000 (64%)]\tLoss: 1.958551\n",
            "Train Epoch: 30 [38400/50000 (77%)]\tLoss: 1.933102\n",
            "Train Epoch: 30 [44800/50000 (90%)]\tLoss: 1.931612\n",
            "\n",
            "Test set: Average loss: -0.4681, Accuracy: 4898/10000 (49%)\n",
            "\n",
            "Train Epoch: 31 [0/50000 (0%)]\tLoss: 1.977793\n",
            "Train Epoch: 31 [6400/50000 (13%)]\tLoss: 1.948213\n",
            "Train Epoch: 31 [12800/50000 (26%)]\tLoss: 1.988830\n",
            "Train Epoch: 31 [19200/50000 (38%)]\tLoss: 1.936432\n",
            "Train Epoch: 31 [25600/50000 (51%)]\tLoss: 2.013820\n",
            "Train Epoch: 31 [32000/50000 (64%)]\tLoss: 1.979896\n",
            "Train Epoch: 31 [38400/50000 (77%)]\tLoss: 1.903709\n",
            "Train Epoch: 31 [44800/50000 (90%)]\tLoss: 1.933831\n",
            "\n",
            "Test set: Average loss: -0.4677, Accuracy: 4898/10000 (49%)\n",
            "\n",
            "Train Epoch: 32 [0/50000 (0%)]\tLoss: 1.933108\n",
            "Train Epoch: 32 [6400/50000 (13%)]\tLoss: 1.900258\n",
            "Train Epoch: 32 [12800/50000 (26%)]\tLoss: 1.951122\n",
            "Train Epoch: 32 [19200/50000 (38%)]\tLoss: 1.932730\n",
            "Train Epoch: 32 [25600/50000 (51%)]\tLoss: 1.996963\n",
            "Train Epoch: 32 [32000/50000 (64%)]\tLoss: 2.022202\n",
            "Train Epoch: 32 [38400/50000 (77%)]\tLoss: 2.051560\n",
            "Train Epoch: 32 [44800/50000 (90%)]\tLoss: 1.979144\n",
            "\n",
            "Test set: Average loss: -0.4665, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 33 [0/50000 (0%)]\tLoss: 1.989831\n",
            "Train Epoch: 33 [6400/50000 (13%)]\tLoss: 2.064919\n",
            "Train Epoch: 33 [12800/50000 (26%)]\tLoss: 1.930352\n",
            "Train Epoch: 33 [19200/50000 (38%)]\tLoss: 1.972798\n",
            "Train Epoch: 33 [25600/50000 (51%)]\tLoss: 1.924425\n",
            "Train Epoch: 33 [32000/50000 (64%)]\tLoss: 2.004682\n",
            "Train Epoch: 33 [38400/50000 (77%)]\tLoss: 1.995116\n",
            "Train Epoch: 33 [44800/50000 (90%)]\tLoss: 2.058135\n",
            "\n",
            "Test set: Average loss: -0.4673, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 34 [0/50000 (0%)]\tLoss: 2.017074\n",
            "Train Epoch: 34 [6400/50000 (13%)]\tLoss: 2.042567\n",
            "Train Epoch: 34 [12800/50000 (26%)]\tLoss: 2.091691\n",
            "Train Epoch: 34 [19200/50000 (38%)]\tLoss: 2.021070\n",
            "Train Epoch: 34 [25600/50000 (51%)]\tLoss: 1.938354\n",
            "Train Epoch: 34 [32000/50000 (64%)]\tLoss: 1.923120\n",
            "Train Epoch: 34 [38400/50000 (77%)]\tLoss: 1.913314\n",
            "Train Epoch: 34 [44800/50000 (90%)]\tLoss: 2.085903\n",
            "\n",
            "Test set: Average loss: -0.4683, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 35 [0/50000 (0%)]\tLoss: 2.046860\n",
            "Train Epoch: 35 [6400/50000 (13%)]\tLoss: 1.998782\n",
            "Train Epoch: 35 [12800/50000 (26%)]\tLoss: 1.989815\n",
            "Train Epoch: 35 [19200/50000 (38%)]\tLoss: 2.051144\n",
            "Train Epoch: 35 [25600/50000 (51%)]\tLoss: 1.951028\n",
            "Train Epoch: 35 [32000/50000 (64%)]\tLoss: 2.136215\n",
            "Train Epoch: 35 [38400/50000 (77%)]\tLoss: 1.990765\n",
            "Train Epoch: 35 [44800/50000 (90%)]\tLoss: 2.036762\n",
            "\n",
            "Test set: Average loss: -0.4674, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 36 [0/50000 (0%)]\tLoss: 1.939533\n",
            "Train Epoch: 36 [6400/50000 (13%)]\tLoss: 1.980330\n",
            "Train Epoch: 36 [12800/50000 (26%)]\tLoss: 1.975036\n",
            "Train Epoch: 36 [19200/50000 (38%)]\tLoss: 2.137521\n",
            "Train Epoch: 36 [25600/50000 (51%)]\tLoss: 1.874046\n",
            "Train Epoch: 36 [32000/50000 (64%)]\tLoss: 2.042137\n",
            "Train Epoch: 36 [38400/50000 (77%)]\tLoss: 1.958242\n",
            "Train Epoch: 36 [44800/50000 (90%)]\tLoss: 2.006666\n",
            "\n",
            "Test set: Average loss: -0.4688, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 37 [0/50000 (0%)]\tLoss: 2.014586\n",
            "Train Epoch: 37 [6400/50000 (13%)]\tLoss: 2.043828\n",
            "Train Epoch: 37 [12800/50000 (26%)]\tLoss: 1.963474\n",
            "Train Epoch: 37 [19200/50000 (38%)]\tLoss: 1.980893\n",
            "Train Epoch: 37 [25600/50000 (51%)]\tLoss: 1.946681\n",
            "Train Epoch: 37 [32000/50000 (64%)]\tLoss: 1.963305\n",
            "Train Epoch: 37 [38400/50000 (77%)]\tLoss: 1.928558\n",
            "Train Epoch: 37 [44800/50000 (90%)]\tLoss: 2.066050\n",
            "\n",
            "Test set: Average loss: -0.4676, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 38 [0/50000 (0%)]\tLoss: 2.066140\n",
            "Train Epoch: 38 [6400/50000 (13%)]\tLoss: 2.015483\n",
            "Train Epoch: 38 [12800/50000 (26%)]\tLoss: 2.054822\n",
            "Train Epoch: 38 [19200/50000 (38%)]\tLoss: 2.067858\n",
            "Train Epoch: 38 [25600/50000 (51%)]\tLoss: 2.008856\n",
            "Train Epoch: 38 [32000/50000 (64%)]\tLoss: 2.067497\n",
            "Train Epoch: 38 [38400/50000 (77%)]\tLoss: 2.020403\n",
            "Train Epoch: 38 [44800/50000 (90%)]\tLoss: 1.977041\n",
            "\n",
            "Test set: Average loss: -0.4670, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 39 [0/50000 (0%)]\tLoss: 1.912854\n",
            "Train Epoch: 39 [6400/50000 (13%)]\tLoss: 1.996298\n",
            "Train Epoch: 39 [12800/50000 (26%)]\tLoss: 1.914708\n",
            "Train Epoch: 39 [19200/50000 (38%)]\tLoss: 1.979572\n",
            "Train Epoch: 39 [25600/50000 (51%)]\tLoss: 2.074291\n",
            "Train Epoch: 39 [32000/50000 (64%)]\tLoss: 2.083015\n",
            "Train Epoch: 39 [38400/50000 (77%)]\tLoss: 1.874424\n",
            "Train Epoch: 39 [44800/50000 (90%)]\tLoss: 1.913438\n",
            "\n",
            "Test set: Average loss: -0.4679, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 40 [0/50000 (0%)]\tLoss: 2.004843\n",
            "Train Epoch: 40 [6400/50000 (13%)]\tLoss: 2.072415\n",
            "Train Epoch: 40 [12800/50000 (26%)]\tLoss: 1.925366\n",
            "Train Epoch: 40 [19200/50000 (38%)]\tLoss: 2.063729\n",
            "Train Epoch: 40 [25600/50000 (51%)]\tLoss: 1.958360\n",
            "Train Epoch: 40 [32000/50000 (64%)]\tLoss: 1.962949\n",
            "Train Epoch: 40 [38400/50000 (77%)]\tLoss: 1.948796\n",
            "Train Epoch: 40 [44800/50000 (90%)]\tLoss: 2.113304\n",
            "\n",
            "Test set: Average loss: -0.4670, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 41 [0/50000 (0%)]\tLoss: 1.996961\n",
            "Train Epoch: 41 [6400/50000 (13%)]\tLoss: 1.947107\n",
            "Train Epoch: 41 [12800/50000 (26%)]\tLoss: 1.968375\n",
            "Train Epoch: 41 [19200/50000 (38%)]\tLoss: 2.050333\n",
            "Train Epoch: 41 [25600/50000 (51%)]\tLoss: 1.965283\n",
            "Train Epoch: 41 [32000/50000 (64%)]\tLoss: 2.010424\n",
            "Train Epoch: 41 [38400/50000 (77%)]\tLoss: 2.068130\n",
            "Train Epoch: 41 [44800/50000 (90%)]\tLoss: 2.027612\n",
            "\n",
            "Test set: Average loss: -0.4676, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 42 [0/50000 (0%)]\tLoss: 2.076207\n",
            "Train Epoch: 42 [6400/50000 (13%)]\tLoss: 2.074920\n",
            "Train Epoch: 42 [12800/50000 (26%)]\tLoss: 2.016434\n",
            "Train Epoch: 42 [19200/50000 (38%)]\tLoss: 2.034779\n",
            "Train Epoch: 42 [25600/50000 (51%)]\tLoss: 2.077795\n",
            "Train Epoch: 42 [32000/50000 (64%)]\tLoss: 1.914020\n",
            "Train Epoch: 42 [38400/50000 (77%)]\tLoss: 2.018707\n",
            "Train Epoch: 42 [44800/50000 (90%)]\tLoss: 2.069606\n",
            "\n",
            "Test set: Average loss: -0.4665, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 43 [0/50000 (0%)]\tLoss: 1.906402\n",
            "Train Epoch: 43 [6400/50000 (13%)]\tLoss: 1.958454\n",
            "Train Epoch: 43 [12800/50000 (26%)]\tLoss: 1.991519\n",
            "Train Epoch: 43 [19200/50000 (38%)]\tLoss: 1.984320\n",
            "Train Epoch: 43 [25600/50000 (51%)]\tLoss: 1.977782\n",
            "Train Epoch: 43 [32000/50000 (64%)]\tLoss: 2.094393\n",
            "Train Epoch: 43 [38400/50000 (77%)]\tLoss: 2.075625\n",
            "Train Epoch: 43 [44800/50000 (90%)]\tLoss: 1.983210\n",
            "\n",
            "Test set: Average loss: -0.4671, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 44 [0/50000 (0%)]\tLoss: 1.961819\n",
            "Train Epoch: 44 [6400/50000 (13%)]\tLoss: 1.945884\n",
            "Train Epoch: 44 [12800/50000 (26%)]\tLoss: 1.941840\n",
            "Train Epoch: 44 [19200/50000 (38%)]\tLoss: 1.997277\n",
            "Train Epoch: 44 [25600/50000 (51%)]\tLoss: 1.960084\n",
            "Train Epoch: 44 [32000/50000 (64%)]\tLoss: 1.886107\n",
            "Train Epoch: 44 [38400/50000 (77%)]\tLoss: 2.078738\n",
            "Train Epoch: 44 [44800/50000 (90%)]\tLoss: 2.069776\n",
            "\n",
            "Test set: Average loss: -0.4678, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 45 [0/50000 (0%)]\tLoss: 1.925924\n",
            "Train Epoch: 45 [6400/50000 (13%)]\tLoss: 1.948611\n",
            "Train Epoch: 45 [12800/50000 (26%)]\tLoss: 2.032721\n",
            "Train Epoch: 45 [19200/50000 (38%)]\tLoss: 1.989178\n",
            "Train Epoch: 45 [25600/50000 (51%)]\tLoss: 1.973507\n",
            "Train Epoch: 45 [32000/50000 (64%)]\tLoss: 2.049965\n",
            "Train Epoch: 45 [38400/50000 (77%)]\tLoss: 1.999143\n",
            "Train Epoch: 45 [44800/50000 (90%)]\tLoss: 1.971706\n",
            "\n",
            "Test set: Average loss: -0.4665, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 46 [0/50000 (0%)]\tLoss: 1.981920\n",
            "Train Epoch: 46 [6400/50000 (13%)]\tLoss: 1.946401\n",
            "Train Epoch: 46 [12800/50000 (26%)]\tLoss: 1.956446\n",
            "Train Epoch: 46 [19200/50000 (38%)]\tLoss: 1.983592\n",
            "Train Epoch: 46 [25600/50000 (51%)]\tLoss: 1.890925\n",
            "Train Epoch: 46 [32000/50000 (64%)]\tLoss: 1.926977\n",
            "Train Epoch: 46 [38400/50000 (77%)]\tLoss: 1.933549\n",
            "Train Epoch: 46 [44800/50000 (90%)]\tLoss: 1.949950\n",
            "\n",
            "Test set: Average loss: -0.4676, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 47 [0/50000 (0%)]\tLoss: 1.981002\n",
            "Train Epoch: 47 [6400/50000 (13%)]\tLoss: 1.981280\n",
            "Train Epoch: 47 [12800/50000 (26%)]\tLoss: 1.835638\n",
            "Train Epoch: 47 [19200/50000 (38%)]\tLoss: 1.971094\n",
            "Train Epoch: 47 [25600/50000 (51%)]\tLoss: 2.047725\n",
            "Train Epoch: 47 [32000/50000 (64%)]\tLoss: 2.017296\n",
            "Train Epoch: 47 [38400/50000 (77%)]\tLoss: 2.019696\n",
            "Train Epoch: 47 [44800/50000 (90%)]\tLoss: 2.027157\n",
            "\n",
            "Test set: Average loss: -0.4667, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 48 [0/50000 (0%)]\tLoss: 2.001019\n",
            "Train Epoch: 48 [6400/50000 (13%)]\tLoss: 2.062178\n",
            "Train Epoch: 48 [12800/50000 (26%)]\tLoss: 1.959943\n",
            "Train Epoch: 48 [19200/50000 (38%)]\tLoss: 1.932116\n",
            "Train Epoch: 48 [25600/50000 (51%)]\tLoss: 1.956427\n",
            "Train Epoch: 48 [32000/50000 (64%)]\tLoss: 1.949491\n",
            "Train Epoch: 48 [38400/50000 (77%)]\tLoss: 1.931803\n",
            "Train Epoch: 48 [44800/50000 (90%)]\tLoss: 1.967776\n",
            "\n",
            "Test set: Average loss: -0.4671, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 49 [0/50000 (0%)]\tLoss: 1.914459\n",
            "Train Epoch: 49 [6400/50000 (13%)]\tLoss: 1.932633\n",
            "Train Epoch: 49 [12800/50000 (26%)]\tLoss: 1.977554\n",
            "Train Epoch: 49 [19200/50000 (38%)]\tLoss: 1.949272\n",
            "Train Epoch: 49 [25600/50000 (51%)]\tLoss: 2.009882\n",
            "Train Epoch: 49 [32000/50000 (64%)]\tLoss: 2.053077\n",
            "Train Epoch: 49 [38400/50000 (77%)]\tLoss: 1.974009\n",
            "Train Epoch: 49 [44800/50000 (90%)]\tLoss: 1.953178\n",
            "\n",
            "Test set: Average loss: -0.4665, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 50 [0/50000 (0%)]\tLoss: 1.928289\n",
            "Train Epoch: 50 [6400/50000 (13%)]\tLoss: 1.899503\n",
            "Train Epoch: 50 [12800/50000 (26%)]\tLoss: 1.949099\n",
            "Train Epoch: 50 [19200/50000 (38%)]\tLoss: 1.967581\n",
            "Train Epoch: 50 [25600/50000 (51%)]\tLoss: 1.894987\n",
            "Train Epoch: 50 [32000/50000 (64%)]\tLoss: 2.020898\n",
            "Train Epoch: 50 [38400/50000 (77%)]\tLoss: 1.967572\n",
            "Train Epoch: 50 [44800/50000 (90%)]\tLoss: 1.987692\n",
            "\n",
            "Test set: Average loss: -0.4672, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 51 [0/50000 (0%)]\tLoss: 2.024538\n",
            "Train Epoch: 51 [6400/50000 (13%)]\tLoss: 1.996382\n",
            "Train Epoch: 51 [12800/50000 (26%)]\tLoss: 2.039902\n",
            "Train Epoch: 51 [19200/50000 (38%)]\tLoss: 2.000179\n",
            "Train Epoch: 51 [25600/50000 (51%)]\tLoss: 1.960127\n",
            "Train Epoch: 51 [32000/50000 (64%)]\tLoss: 1.916522\n",
            "Train Epoch: 51 [38400/50000 (77%)]\tLoss: 2.017404\n",
            "Train Epoch: 51 [44800/50000 (90%)]\tLoss: 1.930591\n",
            "\n",
            "Test set: Average loss: -0.4668, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 52 [0/50000 (0%)]\tLoss: 1.997030\n",
            "Train Epoch: 52 [6400/50000 (13%)]\tLoss: 2.069374\n",
            "Train Epoch: 52 [12800/50000 (26%)]\tLoss: 1.898196\n",
            "Train Epoch: 52 [19200/50000 (38%)]\tLoss: 1.958673\n",
            "Train Epoch: 52 [25600/50000 (51%)]\tLoss: 2.017352\n",
            "Train Epoch: 52 [32000/50000 (64%)]\tLoss: 1.956525\n",
            "Train Epoch: 52 [38400/50000 (77%)]\tLoss: 2.015687\n",
            "Train Epoch: 52 [44800/50000 (90%)]\tLoss: 1.939780\n",
            "\n",
            "Test set: Average loss: -0.4678, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 53 [0/50000 (0%)]\tLoss: 1.997352\n",
            "Train Epoch: 53 [6400/50000 (13%)]\tLoss: 2.001714\n",
            "Train Epoch: 53 [12800/50000 (26%)]\tLoss: 2.075839\n",
            "Train Epoch: 53 [19200/50000 (38%)]\tLoss: 1.974847\n",
            "Train Epoch: 53 [25600/50000 (51%)]\tLoss: 1.925336\n",
            "Train Epoch: 53 [32000/50000 (64%)]\tLoss: 1.971696\n",
            "Train Epoch: 53 [38400/50000 (77%)]\tLoss: 1.872799\n",
            "Train Epoch: 53 [44800/50000 (90%)]\tLoss: 1.994291\n",
            "\n",
            "Test set: Average loss: -0.4668, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 54 [0/50000 (0%)]\tLoss: 1.963419\n",
            "Train Epoch: 54 [6400/50000 (13%)]\tLoss: 1.940360\n",
            "Train Epoch: 54 [12800/50000 (26%)]\tLoss: 1.971715\n",
            "Train Epoch: 54 [19200/50000 (38%)]\tLoss: 1.938052\n",
            "Train Epoch: 54 [25600/50000 (51%)]\tLoss: 1.967695\n",
            "Train Epoch: 54 [32000/50000 (64%)]\tLoss: 1.943006\n",
            "Train Epoch: 54 [38400/50000 (77%)]\tLoss: 2.087692\n",
            "Train Epoch: 54 [44800/50000 (90%)]\tLoss: 1.972766\n",
            "\n",
            "Test set: Average loss: -0.4672, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 55 [0/50000 (0%)]\tLoss: 2.007618\n",
            "Train Epoch: 55 [6400/50000 (13%)]\tLoss: 1.953956\n",
            "Train Epoch: 55 [12800/50000 (26%)]\tLoss: 2.062428\n",
            "Train Epoch: 55 [19200/50000 (38%)]\tLoss: 2.025767\n",
            "Train Epoch: 55 [25600/50000 (51%)]\tLoss: 1.992425\n",
            "Train Epoch: 55 [32000/50000 (64%)]\tLoss: 2.054255\n",
            "Train Epoch: 55 [38400/50000 (77%)]\tLoss: 1.956354\n",
            "Train Epoch: 55 [44800/50000 (90%)]\tLoss: 1.937094\n",
            "\n",
            "Test set: Average loss: -0.4663, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 56 [0/50000 (0%)]\tLoss: 2.022644\n",
            "Train Epoch: 56 [6400/50000 (13%)]\tLoss: 2.008791\n",
            "Train Epoch: 56 [12800/50000 (26%)]\tLoss: 1.935560\n",
            "Train Epoch: 56 [19200/50000 (38%)]\tLoss: 1.972854\n",
            "Train Epoch: 56 [25600/50000 (51%)]\tLoss: 2.047983\n",
            "Train Epoch: 56 [32000/50000 (64%)]\tLoss: 1.964029\n",
            "Train Epoch: 56 [38400/50000 (77%)]\tLoss: 2.059186\n",
            "Train Epoch: 56 [44800/50000 (90%)]\tLoss: 1.918368\n",
            "\n",
            "Test set: Average loss: -0.4676, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 57 [0/50000 (0%)]\tLoss: 2.022675\n",
            "Train Epoch: 57 [6400/50000 (13%)]\tLoss: 2.007723\n",
            "Train Epoch: 57 [12800/50000 (26%)]\tLoss: 1.993610\n",
            "Train Epoch: 57 [19200/50000 (38%)]\tLoss: 1.967015\n",
            "Train Epoch: 57 [25600/50000 (51%)]\tLoss: 1.926501\n",
            "Train Epoch: 57 [32000/50000 (64%)]\tLoss: 2.013171\n",
            "Train Epoch: 57 [38400/50000 (77%)]\tLoss: 1.911736\n",
            "Train Epoch: 57 [44800/50000 (90%)]\tLoss: 1.907274\n",
            "\n",
            "Test set: Average loss: -0.4681, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 58 [0/50000 (0%)]\tLoss: 1.975548\n",
            "Train Epoch: 58 [6400/50000 (13%)]\tLoss: 2.037519\n",
            "Train Epoch: 58 [12800/50000 (26%)]\tLoss: 2.003514\n",
            "Train Epoch: 58 [19200/50000 (38%)]\tLoss: 2.028224\n",
            "Train Epoch: 58 [25600/50000 (51%)]\tLoss: 2.019983\n",
            "Train Epoch: 58 [32000/50000 (64%)]\tLoss: 2.043800\n",
            "Train Epoch: 58 [38400/50000 (77%)]\tLoss: 1.999324\n",
            "Train Epoch: 58 [44800/50000 (90%)]\tLoss: 2.014510\n",
            "\n",
            "Test set: Average loss: -0.4675, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 59 [0/50000 (0%)]\tLoss: 1.959351\n",
            "Train Epoch: 59 [6400/50000 (13%)]\tLoss: 1.939963\n",
            "Train Epoch: 59 [12800/50000 (26%)]\tLoss: 1.943617\n",
            "Train Epoch: 59 [19200/50000 (38%)]\tLoss: 2.054895\n",
            "Train Epoch: 59 [25600/50000 (51%)]\tLoss: 2.002768\n",
            "Train Epoch: 59 [32000/50000 (64%)]\tLoss: 1.985021\n",
            "Train Epoch: 59 [38400/50000 (77%)]\tLoss: 1.990574\n",
            "Train Epoch: 59 [44800/50000 (90%)]\tLoss: 2.018005\n",
            "\n",
            "Test set: Average loss: -0.4672, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 60 [0/50000 (0%)]\tLoss: 2.037506\n",
            "Train Epoch: 60 [6400/50000 (13%)]\tLoss: 1.983768\n",
            "Train Epoch: 60 [12800/50000 (26%)]\tLoss: 2.068506\n",
            "Train Epoch: 60 [19200/50000 (38%)]\tLoss: 2.053523\n",
            "Train Epoch: 60 [25600/50000 (51%)]\tLoss: 1.963633\n",
            "Train Epoch: 60 [32000/50000 (64%)]\tLoss: 2.065016\n",
            "Train Epoch: 60 [38400/50000 (77%)]\tLoss: 1.980054\n",
            "Train Epoch: 60 [44800/50000 (90%)]\tLoss: 1.845286\n",
            "\n",
            "Test set: Average loss: -0.4670, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 61 [0/50000 (0%)]\tLoss: 1.932256\n",
            "Train Epoch: 61 [6400/50000 (13%)]\tLoss: 2.035488\n",
            "Train Epoch: 61 [12800/50000 (26%)]\tLoss: 1.938843\n",
            "Train Epoch: 61 [19200/50000 (38%)]\tLoss: 2.069702\n",
            "Train Epoch: 61 [25600/50000 (51%)]\tLoss: 1.968876\n",
            "Train Epoch: 61 [32000/50000 (64%)]\tLoss: 1.967972\n",
            "Train Epoch: 61 [38400/50000 (77%)]\tLoss: 1.984343\n",
            "Train Epoch: 61 [44800/50000 (90%)]\tLoss: 2.019341\n",
            "\n",
            "Test set: Average loss: -0.4678, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 62 [0/50000 (0%)]\tLoss: 1.989267\n",
            "Train Epoch: 62 [6400/50000 (13%)]\tLoss: 1.963137\n",
            "Train Epoch: 62 [12800/50000 (26%)]\tLoss: 2.039998\n",
            "Train Epoch: 62 [19200/50000 (38%)]\tLoss: 1.998075\n",
            "Train Epoch: 62 [25600/50000 (51%)]\tLoss: 1.921221\n",
            "Train Epoch: 62 [32000/50000 (64%)]\tLoss: 1.918305\n",
            "Train Epoch: 62 [38400/50000 (77%)]\tLoss: 1.865113\n",
            "Train Epoch: 62 [44800/50000 (90%)]\tLoss: 1.916104\n",
            "\n",
            "Test set: Average loss: -0.4676, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 63 [0/50000 (0%)]\tLoss: 2.056155\n",
            "Train Epoch: 63 [6400/50000 (13%)]\tLoss: 1.953516\n",
            "Train Epoch: 63 [12800/50000 (26%)]\tLoss: 1.928697\n",
            "Train Epoch: 63 [19200/50000 (38%)]\tLoss: 2.045891\n",
            "Train Epoch: 63 [25600/50000 (51%)]\tLoss: 2.037256\n",
            "Train Epoch: 63 [32000/50000 (64%)]\tLoss: 2.002706\n",
            "Train Epoch: 63 [38400/50000 (77%)]\tLoss: 2.012563\n",
            "Train Epoch: 63 [44800/50000 (90%)]\tLoss: 2.009284\n",
            "\n",
            "Test set: Average loss: -0.4686, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 64 [0/50000 (0%)]\tLoss: 1.982556\n",
            "Train Epoch: 64 [6400/50000 (13%)]\tLoss: 1.973832\n",
            "Train Epoch: 64 [12800/50000 (26%)]\tLoss: 1.878148\n",
            "Train Epoch: 64 [19200/50000 (38%)]\tLoss: 1.852287\n",
            "Train Epoch: 64 [25600/50000 (51%)]\tLoss: 2.015613\n",
            "Train Epoch: 64 [32000/50000 (64%)]\tLoss: 2.020890\n",
            "Train Epoch: 64 [38400/50000 (77%)]\tLoss: 1.944427\n",
            "Train Epoch: 64 [44800/50000 (90%)]\tLoss: 2.020439\n",
            "\n",
            "Test set: Average loss: -0.4673, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 65 [0/50000 (0%)]\tLoss: 2.001757\n",
            "Train Epoch: 65 [6400/50000 (13%)]\tLoss: 1.893245\n",
            "Train Epoch: 65 [12800/50000 (26%)]\tLoss: 1.984206\n",
            "Train Epoch: 65 [19200/50000 (38%)]\tLoss: 1.959818\n",
            "Train Epoch: 65 [25600/50000 (51%)]\tLoss: 1.982613\n",
            "Train Epoch: 65 [32000/50000 (64%)]\tLoss: 1.982604\n",
            "Train Epoch: 65 [38400/50000 (77%)]\tLoss: 2.039731\n",
            "Train Epoch: 65 [44800/50000 (90%)]\tLoss: 1.984636\n",
            "\n",
            "Test set: Average loss: -0.4687, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 66 [0/50000 (0%)]\tLoss: 2.031975\n",
            "Train Epoch: 66 [6400/50000 (13%)]\tLoss: 1.967067\n",
            "Train Epoch: 66 [12800/50000 (26%)]\tLoss: 2.023001\n",
            "Train Epoch: 66 [19200/50000 (38%)]\tLoss: 2.049488\n",
            "Train Epoch: 66 [25600/50000 (51%)]\tLoss: 2.062017\n",
            "Train Epoch: 66 [32000/50000 (64%)]\tLoss: 1.887771\n",
            "Train Epoch: 66 [38400/50000 (77%)]\tLoss: 1.995577\n",
            "Train Epoch: 66 [44800/50000 (90%)]\tLoss: 1.893359\n",
            "\n",
            "Test set: Average loss: -0.4675, Accuracy: 4899/10000 (49%)\n",
            "\n",
            "Train Epoch: 67 [0/50000 (0%)]\tLoss: 2.069353\n",
            "Train Epoch: 67 [6400/50000 (13%)]\tLoss: 2.051155\n",
            "Train Epoch: 67 [12800/50000 (26%)]\tLoss: 2.028901\n",
            "Train Epoch: 67 [19200/50000 (38%)]\tLoss: 1.980526\n",
            "Train Epoch: 67 [25600/50000 (51%)]\tLoss: 1.965941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6f99d1bd792e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-c5be27981431>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, log_interval, loss_archive)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adadelta.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     86\u001b[0m                        \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                        \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                        weight_decay)\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madadelta\u001b[0;34m(params, grads, square_avgs, acc_deltas, lr, rho, eps, weight_decay)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}